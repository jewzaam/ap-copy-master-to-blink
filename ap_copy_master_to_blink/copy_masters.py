"""
Main copy logic for ap-copy-master-to-blink

Generated By: Claude Code (Claude Sonnet 4.5)
"""

from collections import defaultdict
from datetime import date as date_type
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import logging

from ap_common import get_filtered_metadata, copy_file
from ap_common.progress import progress_iter, ProgressTracker
from ap_common.constants import (
    NORMALIZED_HEADER_CAMERA,
    NORMALIZED_HEADER_GAIN,
    NORMALIZED_HEADER_OFFSET,
    NORMALIZED_HEADER_SETTEMP,
    NORMALIZED_HEADER_READOUTMODE,
    NORMALIZED_HEADER_EXPOSURESECONDS,
    NORMALIZED_HEADER_FILTER,
    NORMALIZED_HEADER_DATE,
    NORMALIZED_HEADER_FILENAME,
    NORMALIZED_HEADER_TYPE,
    TYPE_LIGHT,
    TYPE_MASTER_DARK,
    TYPE_MASTER_BIAS,
    TYPE_MASTER_FLAT,
)

from .config import SUPPORTED_EXTENSIONS
from .matching import (
    determine_required_masters,
    find_candidate_flat_dates,
    find_flat_for_date,
)
from .flat_state import load_state, save_state, get_cutoff, update_cutoff
from .picker import pick_flat_date

logger = logging.getLogger(__name__)

# Set default description width for aligned progress bars
ProgressTracker.set_default_desc_width(20)


def get_date_directory(lights_dir: Path) -> Path:
    """
    Extract DATE directory from lights directory path.

    The lights are in: blink/TARGET/DATE_YYYY-MM-DD/FILTER_X/
    The DATE directory is: blink/TARGET/DATE_YYYY-MM-DD/

    Args:
        lights_dir: Path to directory containing light frames

    Returns:
        Path to DATE directory (parent of lights directory)
    """
    # Check if current directory is a FILTER directory
    if lights_dir.name.startswith("FILTER_"):
        # Parent should be DATE directory
        date_dir = lights_dir.parent
        if date_dir.name.startswith("DATE_"):
            return date_dir

    # If we're already in a DATE directory, return it
    if lights_dir.name.startswith("DATE_"):
        return lights_dir

    # Otherwise, search upward for DATE directory
    current = lights_dir
    while current.parent != current:
        if current.name.startswith("DATE_"):
            return current
        current = current.parent

    # Fallback: use the lights directory itself
    logger.warning(
        f"Could not find DATE directory for {lights_dir}, using lights directory"
    )
    return lights_dir


def scan_blink_directories(
    blink_dir: Path, quiet: bool = False
) -> List[Dict[str, str]]:
    """
    Scan blink directory tree for light frames and read their metadata.

    Args:
        blink_dir: Root blink directory to scan
        quiet: Suppress progress output

    Returns:
        List of metadata dictionaries for all light frames found
    """
    logger.debug(f"Scanning blink directory: {blink_dir}")

    # Get metadata for light frames only (exclude master calibration frames)
    patterns = [rf".*\{ext}$" for ext in SUPPORTED_EXTENSIONS]
    metadata = get_filtered_metadata(
        dirs=[str(blink_dir)],
        filters={NORMALIZED_HEADER_TYPE: TYPE_LIGHT},
        profileFromPath=True,
        patterns=patterns,
        recursive=True,
        printStatus=not quiet,
    )

    if not metadata:
        logger.warning(f"No light frames found in {blink_dir}")
        return []

    # Convert metadata dict to list of dicts
    # (get_filtered_metadata returns {filename: metadata})
    metadata_list = list(metadata.values())

    logger.debug(f"Found {len(metadata_list)} light frames")

    return metadata_list


def group_lights_by_config(
    metadata_list: List[Dict[str, str]],
) -> Dict[Tuple, List[Dict[str, str]]]:
    """
    Group light frames by unique calibration requirements.

    Groups by: camera, gain, offset, settemp, readoutmode, exposureseconds, filter, date

    Args:
        metadata_list: List of metadata dictionaries for light frames

    Returns:
        Dictionary mapping calibration config tuple to list of metadata dicts
    """
    groups: Dict[Tuple, List[Dict[str, str]]] = {}

    for metadata in metadata_list:
        # Create key from calibration-relevant fields
        key = (
            metadata.get(NORMALIZED_HEADER_CAMERA),
            metadata.get(NORMALIZED_HEADER_GAIN),
            metadata.get(NORMALIZED_HEADER_OFFSET),
            metadata.get(NORMALIZED_HEADER_SETTEMP),
            metadata.get(NORMALIZED_HEADER_READOUTMODE),
            metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS),
            metadata.get(NORMALIZED_HEADER_FILTER),
            metadata.get(NORMALIZED_HEADER_DATE),
        )

        if key not in groups:
            groups[key] = []

        groups[key].append(metadata)

    logger.debug(
        f"Grouped {len(metadata_list)} lights into {len(groups)} unique configurations"
    )

    return groups


def _sort_groups_by_date(
    groups: Dict[Tuple, List[Dict[str, str]]],
) -> List[Tuple[Tuple, List[Dict[str, str]]]]:
    """
    Sort configuration groups by date (oldest first).

    Critical for flexible flat matching: lights must be processed in
    chronological order so state file updates cascade correctly.

    The date is the last element (index 7) of the config key tuple.

    Args:
        groups: Dictionary mapping config tuple to list of metadata dicts

    Returns:
        List of (config_key, lights) tuples sorted by date ascending
    """

    def date_sort_key(item: Tuple[Tuple, List[Dict[str, str]]]) -> str:
        config_key = item[0]
        # Date is the last element of the config key tuple
        date_val = config_key[7] if len(config_key) > 7 else ""
        return date_val or ""

    return sorted(groups.items(), key=date_sort_key)


def check_masters_exist(
    date_dir: Path,
    dark: Dict[str, str] = None,
    bias: Dict[str, str] = None,
    flat: Dict[str, str] = None,
) -> Dict[str, bool]:
    """
    Check if specific master calibration files already exist in date directory.

    Args:
        date_dir: Directory to check for existing masters
        dark: Dark master metadata dict (or None if not needed)
        bias: Bias master metadata dict (or None if not needed)
        flat: Flat master metadata dict (or None if not needed)

    Returns:
        Dict with has_dark, has_bias, has_flat boolean flags indicating
        whether the specific files already exist
    """
    result = {"has_dark": False, "has_bias": False, "has_flat": False}

    if not date_dir.exists():
        return result

    # Check if specific master files exist by filename
    if dark:
        dark_name = Path(dark[NORMALIZED_HEADER_FILENAME]).name
        result["has_dark"] = (date_dir / dark_name).exists()

    if bias:
        bias_name = Path(bias[NORMALIZED_HEADER_FILENAME]).name
        result["has_bias"] = (date_dir / bias_name).exists()

    if flat:
        flat_name = Path(flat[NORMALIZED_HEADER_FILENAME]).name
        result["has_flat"] = (date_dir / flat_name).exists()

    return result


def copy_master_to_blink(
    master_metadata: Dict[str, str],
    dest_dir: Path,
    dry_run: bool = False,
) -> bool:
    """
    Copy master calibration frame to blink directory.

    Args:
        master_metadata: Metadata dict for master frame (includes 'filename')
        dest_dir: Destination directory (DATE directory)
        dry_run: If True, log action but don't copy

    Returns:
        True if copied (or would be copied in dry-run), False if skipped
    """
    source_path = Path(master_metadata[NORMALIZED_HEADER_FILENAME])
    dest_path = dest_dir / source_path.name

    if dest_path.exists():
        logger.debug(f"Master already exists, skipping: {dest_path.name}")
        return False

    if dry_run:
        logger.debug(f"[DRY-RUN] Would copy: {source_path.name} -> {dest_dir}")
        return True

    logger.debug(f"Copying: {source_path.name} -> {dest_dir}")
    copy_file(str(source_path), str(dest_path))
    return True


def _collect_filters_by_date(
    groups: Dict[Tuple, List[Dict[str, str]]],
) -> Dict[str, Set[str]]:
    """
    Scan all groups and collect unique filters needed per date.

    Args:
        groups: Output from group_lights_by_config()

    Returns:
        Map: {date_str → set of filter names}
        Example: {"2026-02-07": {"G", "O", "R"}}
    """
    filters_by_date: Dict[str, Set[str]] = defaultdict(set)

    for config_key, lights in groups.items():
        # config_key = (camera, gain, offset, settemp, readoutmode,
        #               exposure, filter, date)
        # Defensive: check tuple length
        if len(config_key) < 8:
            logger.debug(f"Skipping malformed config_key: {config_key}")
            continue

        date = config_key[7]
        filter_name = config_key[6]
        if date and filter_name:
            filters_by_date[date].add(filter_name)

    return dict(filters_by_date)


def _find_candidate_dates_with_all_filters(
    library_dir: Path,
    light_metadata: Dict[str, str],
    required_filters: Set[str],
    cutoff_date: Optional[str],
) -> Dict[str, Dict[str, str]]:
    """
    Find candidate flat dates that have flats for ALL required filters.

    Args:
        library_dir: Path to calibration library
        light_metadata: Metadata from one light (for equipment matching)
        required_filters: Set of filter names needed
        cutoff_date: Exclude dates older than this

    Returns:
        Map: {date_str → flat_metadata}
        Only includes dates with complete filter coverage
    """
    if not required_filters:
        return {}

    # For each required filter, find candidate dates
    candidates_per_filter: Dict[str, Set[str]] = {}

    for filter_name in required_filters:
        # Modify metadata to search for this filter
        search_metadata = dict(light_metadata)
        search_metadata[NORMALIZED_HEADER_FILTER] = filter_name

        # Find all dates with this filter (ignoring date field)
        candidates = find_candidate_flat_dates(
            library_dir, search_metadata, cutoff_date
        )
        candidates_per_filter[filter_name] = set(candidates.keys())

    # Intersect: only dates that have ALL filters
    if candidates_per_filter:
        valid_dates = set.intersection(*candidates_per_filter.values())
    else:
        valid_dates = set()

    # Return map: {date → flat_metadata for one filter (doesn't matter which)}
    result: Dict[str, Dict[str, str]] = {}
    for date_str in valid_dates:
        # Pick first filter's metadata as representative
        first_filter = next(iter(required_filters))
        search_metadata = dict(light_metadata)
        search_metadata[NORMALIZED_HEADER_FILTER] = first_filter
        flat = find_flat_for_date(library_dir, search_metadata, date_str)
        if flat:
            result[date_str] = flat

    return result


def _resolve_flat_for_date(
    library_dir: Path,
    light_metadata: Dict[str, str],
    light_date: str,
    required_filters: Set[str],
    blink_dir_str: str,
    state: Dict[str, str],
    quiet: bool,
    picker_limit: int,
) -> Optional[str]:
    """
    Resolve flat date for a light date (ALL filters).

    Called when no exact-date flat exists for any filter on this date
    and --flat-state is enabled.

    Args:
        library_dir: Path to calibration library
        light_metadata: Metadata from one light (for equipment matching)
        light_date: Light frame date string (YYYY-MM-DD)
        required_filters: Set of filter names needed for this date
        blink_dir_str: Blink directory path string (state file key)
        state: State dictionary (modified in place)
        quiet: If True, skip interactive selection
        picker_limit: Max older/newer dates to show in picker

    Returns:
        Selected flat date string, or None if user chose "rig changed"
    """
    if quiet:
        # Quiet mode: no fallback, no prompting
        filter_names = ", ".join(sorted(required_filters))
        logger.debug(
            f"Quiet mode: skipping flexible flat matching for "
            f"date={light_date}, filters={filter_names}"
        )
        return None

    # Get cutoff from state
    cutoff = get_cutoff(state, blink_dir_str)

    # Find candidates with ALL filters
    candidates = _find_candidate_dates_with_all_filters(
        library_dir, light_metadata, required_filters, cutoff
    )

    # Remove exact date (already tried and failed)
    candidates.pop(light_date, None)

    if not candidates:
        logger.debug(
            f"No candidates with all filters for {light_date} "
            f"(filters: {', '.join(sorted(required_filters))})"
        )
        return None

    # Split into older and newer relative to light date
    try:
        light_date_obj = date_type.fromisoformat(light_date)
    except (ValueError, TypeError):
        logger.warning(f"Invalid light date: {light_date}")
        return None

    older_dates: List[date_type] = []
    newer_dates: List[date_type] = []

    for date_str in sorted(candidates.keys()):
        try:
            d = date_type.fromisoformat(date_str)
        except (ValueError, TypeError):
            continue
        if d < light_date_obj:
            older_dates.append(d)
        elif d > light_date_obj:
            newer_dates.append(d)

    if not older_dates and not newer_dates:
        logger.debug("No older or newer candidate flat dates")
        return None

    # Show interactive picker ONCE for this date
    filter_names = ", ".join(sorted(required_filters))
    selected_date = pick_flat_date(
        light_date,
        f"ALL ({filter_names})",  # Show all filters in prompt
        older_dates,
        newer_dates,
        picker_limit=picker_limit,
    )

    if selected_date is None:
        # User chose "rig changed" - update cutoff to light date
        update_cutoff(state, blink_dir_str, light_date)
        logger.info(f"Rig change recorded at {light_date} (filters: {filter_names})")
        return None

    # User selected a date - update cutoff
    selected_date_str = selected_date.isoformat()
    update_cutoff(state, blink_dir_str, selected_date_str)
    logger.info(
        f"Selected flat date {selected_date_str} for {light_date} "
        f"(filters: {filter_names})"
    )

    return selected_date_str


def process_blink_directory(
    library_dir: Path,
    blink_dir: Path,
    dry_run: bool = False,
    quiet: bool = False,
    scale_darks: bool = False,
    flat_state_path: Optional[Path] = None,
    picker_limit: int = 5,
) -> Dict[str, int]:
    """
    Main orchestration logic to copy masters to blink directories.

    Args:
        library_dir: Path to calibration library
        blink_dir: Path to blink directory tree
        dry_run: If True, log actions but don't copy files
        quiet: Suppress progress output
        scale_darks: If False, only exact exposure match darks are copied.
                    If True, shorter exposure darks with bias are allowed.
        flat_state_path: Path to flat state YAML file. If provided, enables
                        flexible flat date matching with interactive selection.
        picker_limit: Max older/newer flat dates to show in picker (default: 5)

    Returns:
        Dictionary with summary statistics:
        - frame_count: Total number of light frames
        - target_count: Number of unique targets
        - date_count: Number of unique dates
        - filter_count: Number of unique filters
        - darks_needed: Number of DATE directories that need darks
        - darks_present: Number of DATE directories that have darks
        - biases_needed: Number of DATE directories that need biases
        - biases_present: Number of DATE directories that have biases
        - flats_needed: Number of DATE directories that need flats
        - flats_present: Number of DATE directories that have flats
        - configs_processed: Number of unique calibration configs processed
    """
    stats = {
        "frame_count": 0,
        "target_count": 0,
        "date_count": 0,
        "filter_count": 0,
        "darks_needed": 0,
        "darks_present": 0,
        "biases_needed": 0,
        "biases_present": 0,
        "flats_needed": 0,
        "flats_present": 0,
        "configs_processed": 0,
    }

    # Load flat state if enabled
    state: Optional[Dict[str, str]] = None
    if flat_state_path:
        state = load_state(flat_state_path)

    blink_dir_str = str(blink_dir)

    # Scan for light frames
    metadata_list = scan_blink_directories(blink_dir, quiet=quiet)

    if not metadata_list:
        logger.warning("No light frames found to process")
        return stats

    # Extract organizational metrics
    targets = set()
    dates = set()
    filters = set()

    for metadata in metadata_list:
        # Extract target from path (blink_dir/TARGET/DATE/FILTER/file)
        light_path = Path(metadata[NORMALIZED_HEADER_FILENAME])
        try:
            # Get relative path from blink_dir
            rel_path = light_path.relative_to(blink_dir)
            # First component is TARGET
            target = rel_path.parts[0]
            targets.add(target)
        except (ValueError, IndexError):
            # Can't extract target from path
            pass

        # Extract date and filter from metadata
        if NORMALIZED_HEADER_DATE in metadata:
            dates.add(metadata[NORMALIZED_HEADER_DATE])
        if NORMALIZED_HEADER_FILTER in metadata:
            filters.add(metadata[NORMALIZED_HEADER_FILTER])

    stats["frame_count"] = len(metadata_list)
    stats["target_count"] = len(targets)
    stats["date_count"] = len(dates)
    stats["filter_count"] = len(filters)

    # Group by calibration configuration
    groups = group_lights_by_config(metadata_list)
    stats["configs_processed"] = len(groups)

    # Collect filters needed per date (for batch prompting)
    filters_by_date = _collect_filters_by_date(groups)

    # Cache for flat date selections: {light_date → selected_flat_date}
    flat_selections: Dict[str, Optional[str]] = {}

    # Collect warnings to print after progress bar
    warnings = []

    # Track which DATE directories we've processed to avoid duplicate copies
    processed_masters: Dict[Path, Set[str]] = {}

    # Sort groups by date when flexible flat matching is enabled.
    # Critical: oldest-first ordering ensures state file updates cascade
    # correctly—choices for earlier dates inform what's valid for later dates.
    if state is not None:
        sorted_groups = _sort_groups_by_date(groups)
    else:
        sorted_groups = list(groups.items())

    # Pre-prompt for flat dates when flexible matching enabled
    # This prompts ONCE per date (not per filter) for better UX
    if state is not None:
        for light_date in sorted(filters_by_date.keys()):
            filters_needed = filters_by_date[light_date]

            # Check if any group for this date needs flat selection
            needs_selection = False
            representative_light = None

            for config_key, lights in groups.items():
                # Defensive: check tuple length
                if len(config_key) >= 8 and config_key[7] == light_date:  # date field
                    # Use first light as representative for equipment matching
                    light_metadata = lights[0]
                    representative_light = light_metadata

                    # Try exact match first
                    masters = determine_required_masters(
                        library_dir, light_metadata, scale_darks
                    )
                    if masters[TYPE_MASTER_FLAT] is None:
                        needs_selection = True
                        break

            # Prompt once for this date if needed
            if needs_selection and representative_light is not None:
                selected_date = _resolve_flat_for_date(
                    library_dir,
                    representative_light,
                    light_date,
                    filters_needed,
                    blink_dir_str,
                    state,
                    quiet,
                    picker_limit,
                )
                flat_selections[light_date] = selected_date

    # Process each unique configuration
    for config_key, lights in progress_iter(
        sorted_groups,
        desc="Processing configurations",
        unit="configs",
        enabled=not quiet,
        total=len(sorted_groups),
    ):
        # Use first light's metadata as representative for this group
        light_metadata = lights[0]

        logger.debug(
            f"Processing: "
            f"camera={light_metadata.get(NORMALIZED_HEADER_CAMERA)}, "
            f"gain={light_metadata.get(NORMALIZED_HEADER_GAIN)}, "
            f"exposure={light_metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS)}s, "
            f"filter={light_metadata.get(NORMALIZED_HEADER_FILTER)}, "
            f"date={light_metadata.get(NORMALIZED_HEADER_DATE)}"
        )

        # Find required masters
        masters = determine_required_masters(
            library_dir, light_metadata, scale_darks=scale_darks
        )
        dark = masters[TYPE_MASTER_DARK]
        bias = masters[TYPE_MASTER_BIAS]
        flat = masters[TYPE_MASTER_FLAT]

        light_date = light_metadata.get(NORMALIZED_HEADER_DATE, "")
        filter_name = light_metadata.get(NORMALIZED_HEADER_FILTER, "")

        # Flexible flat matching: use cached selection if no exact flat
        if flat is None and state is not None and light_date in flat_selections:
            # Use pre-selected flat date for this light date
            selected_date = flat_selections[light_date]
            if selected_date:
                flat = find_flat_for_date(library_dir, light_metadata, selected_date)
                if not flat:
                    logger.error(
                        f"BUG: Selected date {selected_date} missing flat for "
                        f"filter {filter_name} on {light_date} (should not happen)"
                    )
        elif flat is not None and state is not None:
            # Exact match found: advance cutoff to this date
            if light_date:
                update_cutoff(state, blink_dir_str, light_date)

        # Get all unique DATE directories in this group
        # (lights from multiple targets may share the same calibration config)
        date_dirs = set()
        for light in lights:
            lights_dir = Path(light[NORMALIZED_HEADER_FILENAME]).parent
            date_dir = get_date_directory(lights_dir)
            date_dirs.add(date_dir)

        # Track whether any date_dir has the required masters
        any_dark_present = False
        any_flat_present = False

        # Copy masters to each DATE directory
        for date_dir in date_dirs:
            # Ensure we have a tracking set for this DATE directory
            if date_dir not in processed_masters:
                processed_masters[date_dir] = set()

            # Check if the specific masters we need already exist in date_dir
            existing_masters = check_masters_exist(date_dir, dark, bias, flat)

            # Copy dark (if needed and not already present)
            stats["darks_needed"] += 1
            if existing_masters["has_dark"]:
                logger.debug("Dark already exists in blink for this configuration")
                stats["darks_present"] += 1
                any_dark_present = True
            elif dark:
                dark_name = Path(dark[NORMALIZED_HEADER_FILENAME]).name
                if dark_name not in processed_masters[date_dir]:
                    copy_master_to_blink(dark, date_dir, dry_run)
                    processed_masters[date_dir].add(dark_name)
                stats["darks_present"] += 1
                any_dark_present = True

            # Copy bias (if needed and not already present)
            if bias:
                stats["biases_needed"] += 1
                if existing_masters["has_bias"]:
                    logger.debug("Bias already exists in blink for this configuration")
                    stats["biases_present"] += 1
                else:
                    bias_name = Path(bias[NORMALIZED_HEADER_FILENAME]).name
                    if bias_name not in processed_masters[date_dir]:
                        copy_master_to_blink(bias, date_dir, dry_run)
                        processed_masters[date_dir].add(bias_name)
                    stats["biases_present"] += 1

            # Copy flat (if needed and not already present)
            stats["flats_needed"] += 1
            if existing_masters["has_flat"]:
                logger.debug("Flat already exists in blink for this configuration")
                stats["flats_present"] += 1
                any_flat_present = True
            elif flat:
                flat_name = Path(flat[NORMALIZED_HEADER_FILENAME]).name
                if flat_name not in processed_masters[date_dir]:
                    copy_master_to_blink(flat, date_dir, dry_run)
                    processed_masters[date_dir].add(flat_name)
                stats["flats_present"] += 1
                any_flat_present = True

        # Collect missing master warnings (print after progress bar)
        # Only warn if not found in library AND not already present in any date_dir
        if not dark and not any_dark_present:
            exp = light_metadata.get(NORMALIZED_HEADER_EXPOSURESECONDS)
            warnings.append(
                f"Missing dark for exposure={exp}s "
                f"(run with --debug for search details)"
            )

        if not flat and not any_flat_present:
            filt = light_metadata.get(NORMALIZED_HEADER_FILTER)
            date = light_metadata.get(NORMALIZED_HEADER_DATE)
            warnings.append(
                f"Missing flat for filter={filt}, "
                f"date={date} "
                f"(run with --debug for search details)"
            )

    # Print collected warnings after progress bar completes
    for warning in warnings:
        logger.warning(warning)

    # Save state file (unless dry_run)
    if state is not None and flat_state_path:
        if dry_run:
            logger.debug("Dry run: state file not saved")
        else:
            save_state(flat_state_path, state)

    return stats
